{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a58b0e3",
   "metadata": {},
   "source": [
    "# Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0257bd44",
   "metadata": {},
   "source": [
    "#### What is clustering?\n",
    "\n",
    "Clustering is the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group and dissimilar to the data points in other groups. It is basically a collection of objects on the basis of similarity and dissimilarity between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed769ba",
   "metadata": {},
   "source": [
    "#### How to identify the optimal number of clusters?\n",
    "\n",
    "Using the \"Elbow method\": Plot the error as a function number of clusters. Choose the number of cluster after which the error decreases in a linear manner. Sum of square error or inertia is a well-known metric used for this purpose. TO see this in practise, please refer to [my notebook on k-means](https://github.com/noronhaeyan/machine-learning/blob/main/k-means.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678ea5e5",
   "metadata": {},
   "source": [
    "#### What is the Bias-variance trade-off?\n",
    "\n",
    "As the number of parameters in a model increases, the bias of the fitted model goes down but the variance of the fitted model goes up\n",
    "\n",
    "On the other hand, if the number of parameters in a model is kept low, the variance in the model might not be much but the bias of the model would be high. \n",
    "\n",
    "The gives rise to a U-shaped curve for the model error as a function of the number of parameters. The trade-off in the variance and bias as the number of model parameters are varied is known as the bias-variance trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cce5fd0",
   "metadata": {},
   "source": [
    "#### What is feature engineering? How does it affect the model’s performance? \n",
    "\n",
    "Feature engineering refers to developing some new features by using existing features. Feature engineering can help imporve model performace by 1) increasing accuracy 2) reducing model size 3) Reduce input features by clubbing them together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5215fb3",
   "metadata": {},
   "source": [
    "#### What is overfitting and how can we avoid it?\n",
    "\n",
    "Overfitting happens when the model learns patterns as well as the noises present in the data this leads to high performance on the training data but very low performance for data that the model has not seen earlier. To avoid overfitting there are multiple methods that we can use:\n",
    "\n",
    "1. Early stopping of the model’s training in case of validation training stops increasing but the training keeps going on.\n",
    "2. Using regularization methods like L1 or L2 regularization which is used to penalize the model’s weights to avoid overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6818bd7",
   "metadata": {},
   "source": [
    "#### What is early stopping?\n",
    "\n",
    "In Early Stopping, we stop training the model when the performance of the model on the validation set is getting worse\n",
    "\n",
    "By plotting the error on the training dataset and the validation dataset together, both the errors decrease with a number of iterations until the point where the model starts to overfit. After this point, the training error still decreases but the validation error increases. So, even if training is continued after this point, early stopping essentially returns the set of parameters that were used at this point and so is equivalent to stopping training at that point. So, the final parameters returned will enable the model to have low variance and better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b08f26",
   "metadata": {},
   "source": [
    "#### What is k-fold cross validation?\n",
    "\n",
    "K-fold cross validation is used for estimating prediction error in a given model.\n",
    "\n",
    "The dataset is divided into k subsets or folds. The model is trained and evaluated k times, using a different fold as the validation set each time. Performance metrics from each fold are averaged to estimate the model’s generalization performance. This method aids in model assessment, selection, and hyperparameter tuning, providing a more reliable measure of a model’s effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f701e52e",
   "metadata": {},
   "source": [
    "#### What is the difference between the k-means and k-means++ algorithms?\n",
    "\n",
    "One disadvantage of the K-means algorithm is that it is sensitive to the initialization of the centroids or the mean points. So, if a centroid is initialized to be a “far-off” point, it might just end up with no points associated with it, and at the same time, more than one cluster might end up linked with a single centroid. Similarly, more than one centroid might be initialized into the same cluster resulting in poor clustering.\n",
    "\n",
    "To overcome the above-mentioned drawback we use K-means++. This algorithm ensures a smarter initialization of the centroids and improves the quality of the clustering. Apart from initialization, the rest of the algorithm is the same as the standard K-means algorithm. That is K-means++ is the standard K-means algorithm coupled with a smarter initialization of the centroids.\n",
    "\n",
    "The steps involved are: \n",
    " \n",
    "1. Randomly select the first centroid from the data points.\n",
    "2. For each data point compute its distance from the nearest, previously chosen centroid.\n",
    "3. Select the next centroid from the data points such that the probability of choosing a point as centroid is directly proportional to its distance from the nearest, previously chosen centroid. (i.e. the point having maximum distance from the nearest centroid is most likely to be selected next as a centroid)\n",
    "4. Repeat steps 2 and 3 until k centroids have been sampled\n",
    "\n",
    "Reference: https://www.geeksforgeeks.org/ml-k-means-algorithm/#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845c101c-e83f-45f1-b4ed-d2319315615a",
   "metadata": {},
   "source": [
    "#### Why we cannot use linear regression for a classification task?\n",
    "\n",
    "Linear regresson is sutied for problems when the output values are continous. In classification, the output labels are discrete and bounded.\n",
    "\n",
    "Second reason is that linear regression is sensitive to imbalance data. If the range of input features is too wide, linear regression may miss-classify some data points. Ref: https://towardsdatascience.com/why-linear-regression-is-not-suitable-for-binary-classification-c64457be8e28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd800a84-65f1-4102-9cdd-b2bbd89ab29f",
   "metadata": {},
   "source": [
    "#### Why not mean squared error as a loss function for logistic regression?\n",
    "\n",
    "https://towardsdatascience.com/why-not-mse-as-a-loss-function-for-logistic-regression-589816b5e03c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e35ec3-c3b4-4adc-aad1-de5d96a22ce2",
   "metadata": {},
   "source": [
    "#### Why do we perform normalization?\n",
    "\n",
    "Many statisticla learning algorithms like, pricipal component regression or partial least squares regression require the data matrix to be normalized. \n",
    "\n",
    "In the case of neural networks, normalization can help address the problem of exploding or vanishing gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f05434-0c88-4f9d-993f-232ebe7e0b34",
   "metadata": {},
   "source": [
    "#### What is upsampling anf downsampling? How to work with imbalanced data?\n",
    "\n",
    "To work wit imbalanced data follow one of the items in the below list:\n",
    "1) Upsampling Minority Class\n",
    "2) Downsampling Majority Class\n",
    "3) Generate Synthetic Data\n",
    "4) Combine Upsampling & Downsampling Techniques \n",
    "5) Balanced Class Weight\n",
    "\n",
    "Upsampling may lead to over-fitting of your model. Downsampling may lead to underfitting of your model as we may throw away data points.\n",
    "\n",
    "Code to perform upsampling and downsampling: https://wellsr.com/python/upsampling-and-downsampling-imbalanced-data-in-python/\n",
    "\n",
    "Further reading: https://towardsdatascience.com/5-techniques-to-work-with-imbalanced-data-in-machine-learning-80836d45d30c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1685a84-ef33-446e-9d93-d090371615a7",
   "metadata": {},
   "source": [
    "#### What is data leakage? How to avoid it?\n",
    "\n",
    "Data leakage occurs when information from the target variable leaks into the feature variable, either knowingly or unknowingly. \n",
    "\n",
    "This means that there may be an unusually high correlation between some features and the target variable. When a model is trained on such a dataset, The model may perform very well on test and validation data but may do poorly in production.\n",
    "\n",
    "Source of data leakage can be hard to catch: [This article](https://towardsdatascience.com/data-leakage-in-machine-learning-how-it-can-be-detected-and-minimize-the-risk-8ef4e3a97562) provides a few approaches on how to minimize data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1a2ee4-aa88-42b2-9667-e30621a6974c",
   "metadata": {},
   "source": [
    "#### What are some of the hyperparameters of the random forest regressor which help to avoid overfitting?\n",
    "\n",
    "https://www.geeksforgeeks.org/hyperparameters-of-random-forest-classifier/#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d2ac11-fc92-4aaf-9858-57337edc27af",
   "metadata": {},
   "source": [
    "#### What is Principle Component Analysis (PCA)?\n",
    "\n",
    "PCA(Principal Component Analysis) is an unsupervised machine learning dimensionality reduction technique in which we trade off some information or patterns of the data at the cost of reducing its size significantly. In this algorithm, we try to preserve the variance of the original dataset up to a great extent let’s say 95%. For very high dimensional data sometimes even at the loss of 1% of the variance, we can reduce the data size significantly. By using this algorithm we can perform image compression, visualize high-dimensional data as well as make data visualization easy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a885083d-b65e-46b3-8c10-39581e3bd873",
   "metadata": {},
   "source": [
    "#### Explain the working principle of support vector machine (SVM)?\n",
    "\n",
    "https://www.geeksforgeeks.org/support-vector-machine-algorithm/#\n",
    "\n",
    "Implementation: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07c7b2f-67f5-4f93-b3ca-62171cbaeff5",
   "metadata": {},
   "source": [
    "#### co-relation in hyper param trends in training and test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea00fb0b-6e46-4e6e-825e-1999145dd97b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7a756d-a82a-4224-a7ec-3b19f88d2750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b3442c-83e3-47f1-b72b-be7cf8280e68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd7bd2e-1f79-4dce-a4f5-7da0b71f6d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae7aba4-345f-4a6d-90aa-91d40e205a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e542060-3ca9-41fb-89ec-7a2c4df4b6e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab39ed5-c977-4c5a-95ff-3ed7f786eeaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "353dd08b",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198df06c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74cff584",
   "metadata": {},
   "source": [
    "# General"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45da701d",
   "metadata": {},
   "source": [
    "#### What is a garbage collector?\n",
    "\n",
    "Garbage collector (GC) is a form of automatic memory management.\n",
    "\n",
    "The GC attempts to reclaim memory which was allocated by the program but is no longer referenced.\n",
    "\n",
    "Advantages of GC:\n",
    "1. Frees developers from having to manually release memory\n",
    "2. Avoids memory leaks, in which a program fails to free memory occupied by objects that have become unrechable.\n",
    "\n",
    "Disadvantages of GC:\n",
    "1. GC uses computing resources to decide which memory to free, which impair performance\n",
    "2. Unpredictability in memory management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4654bc6f",
   "metadata": {},
   "source": [
    "#### What is the difference between a CPU and a GPU?\n",
    "\n",
    "The main difference is that CPU is designed to handle a wide-range of tasks quickly, but are limited in the concurrency of tasks.\n",
    "\n",
    "Generally speaking, GPUs are much faster than CPUs at highly parallel simple tasks like multiplying big matrices.\n",
    "\n",
    "GPUs are designed to make rendering of 3D images more efficient."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
